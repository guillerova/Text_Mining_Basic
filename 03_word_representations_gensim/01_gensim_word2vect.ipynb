{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0WwyUKbRHQOV"
   },
   "source": [
    "# Working with embedings using Gensim\n",
    "- Train word2vect embedings from a corpus\n",
    "- Load pretrained embedings\n",
    "- Use embedings to Classify text\n",
    "- Accest to the embedings of spaCy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hy4dhVLUHQOW"
   },
   "outputs": [],
   "source": [
    "# Header\n",
    "import os\n",
    "\n",
    "\n",
    "# solve gensim warning\n",
    "! pip install paramiko\n",
    "\n",
    "import gensim, logging\n",
    "print('Gensim Version: ', gensim.__version__)\n",
    "\n",
    "\n",
    "# Data path\n",
    "data_path = '.'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "avJaU3apHQOa"
   },
   "outputs": [],
   "source": [
    "# Example code to build a word2vect embedings from a corpus\n",
    "\n",
    "# To show in the output the internal messages of the word2vect process\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    " \n",
    "# My little corpus    \n",
    "sentences = [['first', 'sentence'], ['second', 'sentence']]\n",
    "\n",
    "# train word2vec on the two sentences\n",
    "model = gensim.models.Word2Vec(sentences, min_count=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vqsR1EKaHQOc"
   },
   "outputs": [],
   "source": [
    "# Load a more big corpus\n",
    "import nltk\n",
    "nltk.download('brown')\n",
    "\n",
    "from nltk.corpus import brown\n",
    "\n",
    "print('Corpus sentences len:', len(brown.sents()))\n",
    "print('Corpus words len:', len(brown.words()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6GA4HkmlHQOf"
   },
   "outputs": [],
   "source": [
    "# Train a word2 vect model over the new corpus\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(brown.sents(), size=100, window=5, min_count=5, workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xCfe5WxWHQOi"
   },
   "outputs": [],
   "source": [
    "#Persist the model\n",
    "\n",
    "model.save('brown_word2vect_model.bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n56mr7uGHQOk"
   },
   "outputs": [],
   "source": [
    "# Load a trained model\n",
    "\n",
    "model = Word2Vec.load('brown_word2vect_model.bin')  # you can continue training with the loaded model!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SB57OZBXHQOn"
   },
   "outputs": [],
   "source": [
    "# Access to the embedings\n",
    "\n",
    "model.wv['the']  # Vector embeding of a word. Numpy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gd6-usJkHQOp",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Similarity fucntions\n",
    "\n",
    "print('Similars to woman:', model.wv.most_similar_cosmul(positive=['woman']), '\\n')\n",
    "\n",
    "print(\"Indetify the word that doesn't match in a list:\", model.wv.doesnt_match(\"breakfast cereal dinner lunch\".split()), '\\n')\n",
    "\n",
    "print('Words similarity (woman - man):', model.wv.similarity('woman', 'man'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aSm5yHDFHQOt",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the accuracy of the builded embedings over a standar evaluation list of relations\n",
    "\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "analogy_scores = model.wv.evaluate_word_analogies(datapath('questions-words.txt'))\n",
    "\n",
    "print('country - capital corrects', analogy_scores[1][0]['correct'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QLtMImeEKv27"
   },
   "outputs": [],
   "source": [
    "analogy_scores[1][0]['correct']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KeZldA6wHQOw"
   },
   "outputs": [],
   "source": [
    "print('BERLIN GERMANY PARIS FRANCE = ', model.wv.most_similar(positive=['Germany', 'Paris'], negative=['Berlin']), '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vDQlJGjuHQOy"
   },
   "outputs": [],
   "source": [
    "# If you finish to train the model. Save only the embedings and delete model.\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "word_vectors = model.wv\n",
    "\n",
    "fname = 'word_vectors.gz'\n",
    "word_vectors.save(fname)\n",
    "\n",
    "del model\n",
    "\n",
    "# To load:\n",
    "# word_vectors = KeyedVectors.load(fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U-J98Ad_HQO0"
   },
   "outputs": [],
   "source": [
    "# Explore the embedings\n",
    "\n",
    "print('Num of embedings:', len(word_vectors.vocab.keys()), '\\n')\n",
    "\n",
    "print('Sample of words available (20 first):', list(word_vectors.vocab.keys())[:20], '\\n')\n",
    "\n",
    "print('Vocab word attributes for \"Oregon\" word:', word_vectors.vocab['Oregon'], '\\n')\n",
    "\n",
    "print('Word embedings for \"Oregon\" word:', word_vectors['Oregon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0b8N44ctHQO2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Vocabulary frequency. List the words with freq > 1000\n",
    "\n",
    "for k in word_vectors.vocab.keys():\n",
    "    if word_vectors.vocab[k].count > 1000:\n",
    "        print(k, word_vectors.vocab[k].count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wpQjv1TbHQO5"
   },
   "source": [
    "## Load pretrained vectors and use it\n",
    "\n",
    "Load pretrained vectors (300Mb) from\n",
    " http://nlpserver2.inf.ufrgs.br/alexandres/vectors/lexvec.enwiki%2bnewscrawl.300d.W.pos.vectors.gz \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tWMXPFO-H9z_"
   },
   "outputs": [],
   "source": [
    "# Download vectors into colab\n",
    "! wget https://www.dropbox.com/s/kguufyc2xcdi8yk/lexvec.enwiki%2Bnewscrawl.300d.W.pos.vectors.gz\n",
    "\n",
    "! gunzip lexvec.enwiki+newscrawl.300d.W.pos.vectors.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qGhxyB4-HQO6"
   },
   "outputs": [],
   "source": [
    "# Load pretrained embedings\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(os.path.join(data_path, 'lexvec.enwiki+newscrawl.300d.W.pos.vectors'),\n",
    "                                          unicode_errors='ignore')\n",
    "\n",
    "print('Sample of one embeding')\n",
    "dog = model['dog']\n",
    "print('Shape of one embeding:', dog.shape)\n",
    "print('First 10 embedings of \"dog\":', dog[:10], '\\n')\n",
    "\n",
    "\n",
    "# Some predefined functions that show content related information for given words\n",
    "print('woman + king - man = ', model.most_similar(positive=['woman', 'king'], negative=['man']), '\\n')\n",
    "\n",
    "print(\"Doesn't match:\", model.doesnt_match(\"breakfast cereal dinner lunch\".split()), '\\n')\n",
    "\n",
    "print('Similarity woman - man:', model.similarity('woman', 'man'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OO5hwBPaHQO8"
   },
   "outputs": [],
   "source": [
    "# Test the accuracy of this model\n",
    "analogy_scores = model.wv.evaluate_word_analogies(datapath('questions-words.txt'))\n",
    "\n",
    "print('country - capital corrects', analogy_scores[1][0]['correct'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mSXz9ajzHQO-"
   },
   "outputs": [],
   "source": [
    "# use in a model\n",
    "import numpy as np\n",
    "\n",
    "# Load data. Sentiment model in movies reviews.\n",
    "# Reference: http://www.aclweb.org/anthology/P11-1015 \n",
    "\n",
    "! wget https://s3-eu-west-1.amazonaws.com/text-mining-course/sentiment_corpus.zip\n",
    "! unzip sentiment_corpus.zip\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "McLVesfBLVWN"
   },
   "outputs": [],
   "source": [
    "X_trn = np.load(os.path.join(data_path, 'sentiment_X_trn.npy')) \n",
    "X_tst = np.load(os.path.join(data_path, 'sentiment_X_tst.npy'))\n",
    "y_trn = np.load(os.path.join(data_path, 'sentiment_y_trn.npy')) # 1: pos, 0:neg\n",
    "y_tst = np.load(os.path.join(data_path, 'sentiment_y_tst.npy')) # 1: pos, 0:neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3NvLOMWzHQPB"
   },
   "outputs": [],
   "source": [
    "print(X_trn.shape)\n",
    "print(X_trn[:2])\n",
    "print(y_trn[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "97XtN7zzYiGc"
   },
   "outputs": [],
   "source": [
    "model.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Grn4Uw5dHQPE"
   },
   "outputs": [],
   "source": [
    "# Represent each sentence by the average embeding of the words located in the embedings dictionary\n",
    "\n",
    "def encode_text(corpus, model):\n",
    "    '''\n",
    "    Function to encode text sentences into one embedding by sentence\n",
    "        input: A list of sentences (corpus) and a embeddings model (model)\n",
    "        output: One embedding for each sentence (average of embeddings of the words in the sentence)\n",
    "    '''\n",
    "    features_list = []\n",
    "    for s in corpus:\n",
    "        features = []\n",
    "        for t in s:\n",
    "            if str.upper(t) in model.vocab.keys():\n",
    "                features += [model[str.upper(t)]]\n",
    "        features_list += [np.mean(features, axis=0)] \n",
    "    return np.array(features_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SXd0NMaGHQPI"
   },
   "outputs": [],
   "source": [
    "# Check embedings shape\n",
    "embeds_trn = encode_text(X_trn, model)\n",
    "print('Embeds trn shape:', embeds_trn.shape)\n",
    "\n",
    "embeds_tst = encode_text(X_tst, model)\n",
    "print('Embeds tst shape:', embeds_tst.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dd0aL9_THQPL"
   },
   "outputs": [],
   "source": [
    "# Build a model and evaluate it\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Train\n",
    "text_clf_svm = LinearSVC()\n",
    "text_clf_svm.fit(embeds_trn, y_trn)\n",
    "\n",
    "#Evaluate test data\n",
    "predicted = text_clf_svm.predict(embeds_tst)\n",
    "print('Test accuracy:', np.mean(predicted == y_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rk_EDPEgHQPN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aui8vqQZHQPP"
   },
   "source": [
    "# Word embedings in spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iiYPsRtqLjFh"
   },
   "outputs": [],
   "source": [
    "# Install modelo with embeddings\n",
    "! python -m spacy download en_core_web_md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-E3INiT3HQPP"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load a model with embedings . The small models don't have embedings\n",
    "import en_core_web_md\n",
    "nlp = en_core_web_md.load()\n",
    "print('Model loaded!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kdMUScSJHQPR"
   },
   "outputs": [],
   "source": [
    "# Chech if the token has a vector.\n",
    "tokens = nlp(u'dog cat banana bibliopole')\n",
    "\n",
    "for token in tokens:\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DV9Ow_R3HQPT"
   },
   "outputs": [],
   "source": [
    "# For a given document, calculate similarity between 'apples' and 'oranges' and 'boots' and 'hippos'\n",
    "doc = nlp(u\"Apples and oranges are similar. Boots and hippos aren't.\")\n",
    "apples = doc[0]\n",
    "oranges = doc[2]\n",
    "boots = doc[6]\n",
    "hippos = doc[8]\n",
    "print('Similarity between words:')\n",
    "print('apples vs oranges: ', apples.similarity(oranges))\n",
    "print('boots vs hippos:', boots.similarity(hippos))\n",
    "\n",
    "print()\n",
    "print('Similarity between a word and a sentence:')\n",
    "# Print similarity between sentence and word 'fruit'\n",
    "apples_sent, boots_sent = doc.sents\n",
    "fruit = doc.vocab[u'fruit']\n",
    "print('apples sentence vs fruit word: ', apples_sent.similarity(fruit))\n",
    "print('boots sentence vs fruit word:', boots_sent.similarity(fruit))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nuax6xwDHQPV"
   },
   "outputs": [],
   "source": [
    "# Show a vector\n",
    "apples.vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YZa0faF1HQPX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "03_gensim_word2vect.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
