{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_NLP_whit_NLTK.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "LGTxug21-L7_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# NLP processing whit NLTK\n",
        "- Load an manage corpus\n",
        "- Tokenize\n",
        "- POS\n",
        "- Lemmatization and stemming\n",
        "- NER\n",
        "- Standford NLP engine\n",
        "- Pipelines fo EN and ES"
      ]
    },
    {
      "metadata": {
        "id": "VZzMtpO4-L8A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "66dbd5f7-091a-4cfc-f8c4-15cefe3b747d"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import os\n",
        "\n",
        "nltk.__version__\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.2.5'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "8nHhsF-I-L8F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "2720bf3c-ffe2-476b-9945-1d65637d3414"
      },
      "cell_type": "code",
      "source": [
        "# The linguistic resources must be instaled in the nltk_data dir.\n",
        "\n",
        "# Check the nltk_data dir path\n",
        "print(nltk.data.path)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['/root/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data', '/usr/nltk_data', '/usr/lib/nltk_data']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "y1TKKGPp-L8I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "0307b992-b434-4300-e948-980210fd387a"
      },
      "cell_type": "code",
      "source": [
        "# If you plan to use another dir, add it\n",
        "\n",
        "new_data_path='/tmp'\n",
        "nltk.data.path.append(new_data_path)\n",
        "\n",
        "print(nltk.data.path)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['/root/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data', '/usr/nltk_data', '/usr/lib/nltk_data', '/tmp']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ygrlZIhQ-L8L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "cb1dcf92-0116-46ee-de30-55e55eac2aad"
      },
      "cell_type": "code",
      "source": [
        "# Download resources of NLTK data\n",
        "nltk.download('punkt') # Punkt Tokenizer Models\n",
        "\n",
        "# List of available resources here: http://www.nltk.org/nltk_data/ "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "-8Q__TRH-L8N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load and manage a corpus"
      ]
    },
    {
      "metadata": {
        "id": "7qhFmPvz-L8O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "b110c3d2-f649-4f3f-eda1-5e79f2292013"
      },
      "cell_type": "code",
      "source": [
        "# Download the Brown corpus\n",
        "nltk.download('brown') \n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "2b2vj7UQ-L8Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "869351a6-b137-4056-c659-21762c4b5605"
      },
      "cell_type": "code",
      "source": [
        "# Then you can import it\n",
        "from nltk.corpus import brown\n",
        "\n",
        "print('Corpus len:', len(brown.words()))\n",
        "\n",
        "print('The first 10 words:', brown.words()[0:10])\n",
        "\n",
        "print('The first 10 tagged words:', brown.tagged_words()[0:10])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Corpus len: 1161192\n",
            "The first 10 words: ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of']\n",
            "The first 10 tagged words: [('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eQbYAhJK-L8T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5xONMu6G-L8V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Tokenization\n",
        "- Tokenize sentences\n",
        "- Tokenize words"
      ]
    },
    {
      "metadata": {
        "id": "KmS2VOTy-L8V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "2714ed08-6ef5-4222-a96a-f96a6e762f02"
      },
      "cell_type": "code",
      "source": [
        "# Use the predefined sentence tokenizer.\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text = \"this’s a sent tokenize test. tis is sennt two. is this sent three? sent 4 is cool! Now it’s your turn.\"\n",
        "sent_tokenize_list = sent_tokenize(text)\n",
        "\n",
        "print(sent_tokenize_list)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['this’s a sent tokenize test.', 'tis is sennt two.', 'is this sent three?', 'sent 4 is cool!', 'Now it’s your turn.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "riY_EeaP-L8Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "02f7fc44-ae51-48e9-e660-e65605811e69"
      },
      "cell_type": "code",
      "source": [
        "# Use explicity the punkt english sentence tokenizer\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "tokenizer.tokenize(text)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this’s a sent tokenize test.',\n",
              " 'tis is sennt two.',\n",
              " 'is this sent three?',\n",
              " 'sent 4 is cool!',\n",
              " 'Now it’s your turn.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "zlus5lAW-L8b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3090ca02-ee63-4ca0-b801-e9bb7f358e58"
      },
      "cell_type": "code",
      "source": [
        "# Use explicity the spanish sentence tokenizer\n",
        "spanish_tokenizer = nltk.data.load('tokenizers/punkt/spanish.pickle')\n",
        "spanish_tokenizer.tokenize('¡Buenos días! ¿Estas bien?')\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['¡Buenos días!', '¿Estas bien?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "cAMj2XiC-L8e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "a9af37b7-b21e-493e-9512-d34fb126d14f"
      },
      "cell_type": "code",
      "source": [
        "# List of sentence tokenizers available in the punkt module\n",
        "os.listdir(nltk.data.path[0]+'/tokenizers/punkt/')\n",
        "\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['swedish.pickle',\n",
              " 'norwegian.pickle',\n",
              " 'greek.pickle',\n",
              " 'german.pickle',\n",
              " 'spanish.pickle',\n",
              " 'polish.pickle',\n",
              " 'portuguese.pickle',\n",
              " 'italian.pickle',\n",
              " 'README',\n",
              " 'danish.pickle',\n",
              " 'slovene.pickle',\n",
              " 'french.pickle',\n",
              " 'turkish.pickle',\n",
              " 'finnish.pickle',\n",
              " 'estonian.pickle',\n",
              " 'english.pickle',\n",
              " 'dutch.pickle',\n",
              " 'PY3',\n",
              " 'czech.pickle']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "feJZIpRI-L8g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "4c2c265a-b592-4df9-8a88-3a225935d3c5"
      },
      "cell_type": "code",
      "source": [
        "# Word tokenizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "print(word_tokenize('Hello World!'))\n",
        "print(word_tokenize(\"Can't is a contraction.\"))\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hello', 'World', '!']\n",
            "['Ca', \"n't\", 'is', 'a', 'contraction', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WctPKfyV-L8k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "0e1c7502-d84f-4590-89c0-7277f8b15687"
      },
      "cell_type": "code",
      "source": [
        "# Others word tokenizers\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "tokenizer = WhitespaceTokenizer()\n",
        "print(tokenizer.tokenize(\"Can't is a contraction.\"))\n",
        "\n",
        "\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "tokenizer = WordPunctTokenizer()\n",
        "print(tokenizer.tokenize(\"Can't is a contraction.\"))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"Can't\", 'is', 'a', 'contraction.']\n",
            "['Can', \"'\", 't', 'is', 'a', 'contraction', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Gs60ap9t-L8n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "9150d126-4934-4a71-e445-eb58a560eb96"
      },
      "cell_type": "code",
      "source": [
        "# word_tokenize must be used over sentences of the text\n",
        "\n",
        "text = \"El Dpto de RR.HH. ha lanzado 1.000 ofertas de trabajo en Buenos Aires. \" \\\n",
        "       \"3,25€ perdidos en Madrid el 2/11/2017. \"\\\n",
        "       \"Las herramientas [h1 y h2] son compatibles.\"\n",
        "\n",
        "sent_tok = nltk.tokenize.load('tokenizers/punkt/spanish.pickle')\n",
        "word_tok = nltk.tokenize.TreebankWordTokenizer()\n",
        "\n",
        "\n",
        "sents = sent_tok.tokenize(text)\n",
        "\n",
        "tokens = []\n",
        "for s in sents:\n",
        "    tokens += word_tok.tokenize(s)\n",
        "    \n",
        "print(tokens)\n",
        "\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['El', 'Dpto', 'de', 'RR.HH', '.', 'ha', 'lanzado', '1.000', 'ofertas', 'de', 'trabajo', 'en', 'Buenos', 'Aires', '.', '3,25€', 'perdidos', 'en', 'Madrid', 'el', '2/11/2017', '.', 'Las', 'herramientas', '[', 'h1', 'y', 'h2', ']', 'son', 'compatibles', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hN1xXpf0-L8r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "3bbdceab-26a7-49a9-a760-8ca92ba50e33"
      },
      "cell_type": "code",
      "source": [
        "# Stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "english_stops = set(stopwords.words('english'))\n",
        "\n",
        "words = [\"Can't\", 'is', 'a', 'contraction']\n",
        "words_clean = [word for word in words if word not in english_stops]\n",
        "print(words_clean)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[\"Can't\", 'contraction']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UDQBG6Wu-L8u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "34ab0e75-3c29-44e8-e93a-b16514030410"
      },
      "cell_type": "code",
      "source": [
        "print('Available stopwords lists:', stopwords.fileids())\n",
        "\n",
        "print('Stop words spanish:', stopwords.words('spanish'))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Available stopwords lists: ['arabic', 'azerbaijani', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish', 'turkish']\n",
            "Stop words spanish: ['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'para', 'con', 'no', 'una', 'su', 'al', 'lo', 'como', 'más', 'pero', 'sus', 'le', 'ya', 'o', 'este', 'sí', 'porque', 'esta', 'entre', 'cuando', 'muy', 'sin', 'sobre', 'también', 'me', 'hasta', 'hay', 'donde', 'quien', 'desde', 'todo', 'nos', 'durante', 'todos', 'uno', 'les', 'ni', 'contra', 'otros', 'ese', 'eso', 'ante', 'ellos', 'e', 'esto', 'mí', 'antes', 'algunos', 'qué', 'unos', 'yo', 'otro', 'otras', 'otra', 'él', 'tanto', 'esa', 'estos', 'mucho', 'quienes', 'nada', 'muchos', 'cual', 'poco', 'ella', 'estar', 'estas', 'algunas', 'algo', 'nosotros', 'mi', 'mis', 'tú', 'te', 'ti', 'tu', 'tus', 'ellas', 'nosotras', 'vosostros', 'vosostras', 'os', 'mío', 'mía', 'míos', 'mías', 'tuyo', 'tuya', 'tuyos', 'tuyas', 'suyo', 'suya', 'suyos', 'suyas', 'nuestro', 'nuestra', 'nuestros', 'nuestras', 'vuestro', 'vuestra', 'vuestros', 'vuestras', 'esos', 'esas', 'estoy', 'estás', 'está', 'estamos', 'estáis', 'están', 'esté', 'estés', 'estemos', 'estéis', 'estén', 'estaré', 'estarás', 'estará', 'estaremos', 'estaréis', 'estarán', 'estaría', 'estarías', 'estaríamos', 'estaríais', 'estarían', 'estaba', 'estabas', 'estábamos', 'estabais', 'estaban', 'estuve', 'estuviste', 'estuvo', 'estuvimos', 'estuvisteis', 'estuvieron', 'estuviera', 'estuvieras', 'estuviéramos', 'estuvierais', 'estuvieran', 'estuviese', 'estuvieses', 'estuviésemos', 'estuvieseis', 'estuviesen', 'estando', 'estado', 'estada', 'estados', 'estadas', 'estad', 'he', 'has', 'ha', 'hemos', 'habéis', 'han', 'haya', 'hayas', 'hayamos', 'hayáis', 'hayan', 'habré', 'habrás', 'habrá', 'habremos', 'habréis', 'habrán', 'habría', 'habrías', 'habríamos', 'habríais', 'habrían', 'había', 'habías', 'habíamos', 'habíais', 'habían', 'hube', 'hubiste', 'hubo', 'hubimos', 'hubisteis', 'hubieron', 'hubiera', 'hubieras', 'hubiéramos', 'hubierais', 'hubieran', 'hubiese', 'hubieses', 'hubiésemos', 'hubieseis', 'hubiesen', 'habiendo', 'habido', 'habida', 'habidos', 'habidas', 'soy', 'eres', 'es', 'somos', 'sois', 'son', 'sea', 'seas', 'seamos', 'seáis', 'sean', 'seré', 'serás', 'será', 'seremos', 'seréis', 'serán', 'sería', 'serías', 'seríamos', 'seríais', 'serían', 'era', 'eras', 'éramos', 'erais', 'eran', 'fui', 'fuiste', 'fue', 'fuimos', 'fuisteis', 'fueron', 'fuera', 'fueras', 'fuéramos', 'fuerais', 'fueran', 'fuese', 'fueses', 'fuésemos', 'fueseis', 'fuesen', 'sintiendo', 'sentido', 'sentida', 'sentidos', 'sentidas', 'siente', 'sentid', 'tengo', 'tienes', 'tiene', 'tenemos', 'tenéis', 'tienen', 'tenga', 'tengas', 'tengamos', 'tengáis', 'tengan', 'tendré', 'tendrás', 'tendrá', 'tendremos', 'tendréis', 'tendrán', 'tendría', 'tendrías', 'tendríamos', 'tendríais', 'tendrían', 'tenía', 'tenías', 'teníamos', 'teníais', 'tenían', 'tuve', 'tuviste', 'tuvo', 'tuvimos', 'tuvisteis', 'tuvieron', 'tuviera', 'tuvieras', 'tuviéramos', 'tuvierais', 'tuvieran', 'tuviese', 'tuvieses', 'tuviésemos', 'tuvieseis', 'tuviesen', 'teniendo', 'tenido', 'tenida', 'tenidos', 'tenidas', 'tened']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1zMLltnL-L8x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Regular expressions\n",
        "- https://www.regular-expressions.info/ \n",
        "- https://www.regextester.com/ "
      ]
    },
    {
      "metadata": {
        "id": "ufteLf4M-L8y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "36ca3c5f-b594-403f-da15-8d2a98f528db"
      },
      "cell_type": "code",
      "source": [
        "# Regular expressions to clean text\n",
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    text_clean = text\n",
        "    \n",
        "    # Lowercase\n",
        "    text_clean = str.lower(text_clean)\n",
        "    \n",
        "    # Replace numbers integer, float negatives. Not replace 1 digit numbers\n",
        "    text_clean = re.sub(\"[-]?[\\d]+[.]?[\\d]+\", \"DIGIT\", text_clean)\n",
        "\n",
        "    # Delete characters [ ] { } ⋅ −\n",
        "    text_clean = re.sub('[\\[\\]/{}⋅−]+', ' ', text_clean)\n",
        "    \n",
        "    # Other cleaning options \n",
        "    \n",
        "    \n",
        "    return text_clean\n",
        "\n",
        "text = \"Los datos son 23.5 y -12.8 [Medidos en unidades].\"\n",
        "print(text)\n",
        "print(clean_text(text))\n",
        "\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Los datos son 23.5 y -12.8 [Medidos en unidades].\n",
            "los datos son DIGIT y DIGIT  medidos en unidades .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MC679EFY-L80",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WgpeUuxz-L82",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# POS"
      ]
    },
    {
      "metadata": {
        "id": "yWWfG5XX-L82",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "8e3521b0-9f11-47e6-a0d1-e344afeeb403"
      },
      "cell_type": "code",
      "source": [
        "# Download the POS model\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# The POS model must be applied over word tokenized text\n",
        "text = nltk.word_tokenize(\"dive into NLTK: Part-of-speech tagging and POS Tagger\")\n",
        "print(text)\n",
        "# Use the recommended part of speech tagger\n",
        "print(nltk.pos_tag(text))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "['dive', 'into', 'NLTK', ':', 'Part-of-speech', 'tagging', 'and', 'POS', 'Tagger']\n",
            "[('dive', 'NN'), ('into', 'IN'), ('NLTK', 'NNP'), (':', ':'), ('Part-of-speech', 'JJ'), ('tagging', 'NN'), ('and', 'CC'), ('POS', 'NNP'), ('Tagger', 'NNP')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wsLEr73m-L85",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "b470aad3-aa3c-4c6c-d14c-a2ce240e1804"
      },
      "cell_type": "code",
      "source": [
        "# Understand the tagset of the POS model\n",
        "nltk.download('tagsets')\n",
        "\n",
        "print(nltk.help.upenn_tagset('JJ'))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Unzipping help/tagsets.zip.\n",
            "JJ: adjective or numeral, ordinal\n",
            "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
            "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
            "    multilingual multi-disciplinary ...\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vlNGkQaU-L88",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Steming"
      ]
    },
    {
      "metadata": {
        "id": "-Z00Xc2S-L88",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "163509da-8ad7-4099-9af4-abc7cc5beede"
      },
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "porter_stemmer = PorterStemmer()\n",
        "print('Porter stemmer:')\n",
        "print('---------------')\n",
        "print(porter_stemmer.stem('maximum'))\n",
        "print(porter_stemmer.stem('presumably'))\n",
        "print(porter_stemmer.stem('multiply'))\n",
        "print(porter_stemmer.stem('provision'))\n",
        "print(porter_stemmer.stem('saying'),'\\n')\n",
        "\n",
        "\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "\n",
        "lancaster_stemmer = LancasterStemmer()\n",
        "print('Lancaster stemmer:')\n",
        "print('---------------')\n",
        "print(lancaster_stemmer.stem('maximum'))\n",
        "print(lancaster_stemmer.stem('presumably'))\n",
        "print(lancaster_stemmer.stem('multiply'))\n",
        "print(lancaster_stemmer.stem('provision'))\n",
        "print(lancaster_stemmer.stem('saying'),'\\n')\n",
        "\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "snowball_stemmer = SnowballStemmer(\"english\")\n",
        "print('Snowball stemmer:')\n",
        "print('---------------')\n",
        "print(snowball_stemmer.stem('maximum'))\n",
        "print(snowball_stemmer.stem('presumably'))\n",
        "print(snowball_stemmer.stem('multiply'))\n",
        "print(snowball_stemmer.stem('provision'))\n",
        "print(snowball_stemmer.stem('saying'))\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Porter stemmer:\n",
            "---------------\n",
            "maximum\n",
            "presum\n",
            "multipli\n",
            "provis\n",
            "say \n",
            "\n",
            "Lancaster stemmer:\n",
            "---------------\n",
            "maxim\n",
            "presum\n",
            "multiply\n",
            "provid\n",
            "say \n",
            "\n",
            "Snowball stemmer:\n",
            "---------------\n",
            "maximum\n",
            "presum\n",
            "multipli\n",
            "provis\n",
            "say\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5bo0Bzed-L9A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "8208d0a4-b542-43a8-ad61-18c6979ec301"
      },
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "print('Available languages in snowball stemmer:', \" \".join(SnowballStemmer.languages))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Available languages in snowball stemmer: arabic danish dutch english finnish french german hungarian italian norwegian porter portuguese romanian russian spanish swedish\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6EFVlG19-L9E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c42c8e61-954d-4a47-c556-044f50949ba6"
      },
      "cell_type": "code",
      "source": [
        "#  Test for the spanish language\n",
        "from nltk.stem.snowball import SpanishStemmer\n",
        "stemmer = SpanishStemmer()\n",
        "stemmer.stem(\"Semanalmente\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'semanal'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "EfvZGjiD-L9G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kBVEmWYf-L9I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Lemmatizer"
      ]
    },
    {
      "metadata": {
        "id": "qRXJfw2I-L9I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "c6329e70-7a9b-48af-d76e-442662c24703"
      },
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet') \n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "print('Sample of lemmatizations:')\n",
        "print(wordnet_lemmatizer.lemmatize('dogs'))\n",
        "print(wordnet_lemmatizer.lemmatize('churches'))\n",
        "print(wordnet_lemmatizer.lemmatize('abaci'))\n",
        "print(wordnet_lemmatizer.lemmatize('are'), '\\n')\n",
        "\n",
        "# Lemmatization with POS\n",
        "\n",
        "print('Lemma of \"is\", no POS:', wordnet_lemmatizer.lemmatize('is'))\n",
        "\n",
        "print('Lemma of \"is\", whit POS:', wordnet_lemmatizer.lemmatize('is', pos='v'))\n",
        "print('Lemma of \"are\", whit POS:', wordnet_lemmatizer.lemmatize('are', pos='v'))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "Sample of lemmatizations:\n",
            "dog\n",
            "church\n",
            "abacus\n",
            "are \n",
            "\n",
            "Lemma of \"is\", no POS: is\n",
            "Lemma of \"is\", whit POS: be\n",
            "Lemma of \"are\", whit POS: be\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZhvJXGQ9-L9K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "14WvsSVc-L9M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Integrated process"
      ]
    },
    {
      "metadata": {
        "id": "b38DXk_N-L9M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Integrating the treebank POS tags to wordnet compatible pos tags\n",
        "# - The recomended POS use different codes for the POS labels that the wordnet lemmatizer needs\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return None # for easy if-statement "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UWLNBSsd-L9N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "018c2a55-6dd5-4dae-9980-10a97fe5d7ca"
      },
      "cell_type": "code",
      "source": [
        "text = \"You are good friends. We had two houses that are bigger!\"\n",
        "\n",
        "# Tokenize into sentences\n",
        "sents = sent_tok.tokenize(text)\n",
        "\n",
        "# for each sentence\n",
        "# - Tokenize words\n",
        "# - POS model\n",
        "# - Lemmatizer whit POS\n",
        "tokens = []\n",
        "tokens_stem = []\n",
        "for s in sents:\n",
        "    t = word_tok.tokenize(s)\n",
        "    tagged = nltk.pos_tag(t)\n",
        "    tokens += [t]\n",
        "    lemma_list = []\n",
        "    for word, tag in tagged:\n",
        "        wntag = get_wordnet_pos(tag)\n",
        "        if wntag is None: # not supply tag in case of None\n",
        "            lemma = wordnet_lemmatizer.lemmatize(word) \n",
        "        else:\n",
        "            lemma = wordnet_lemmatizer.lemmatize(word, pos=wntag) \n",
        "        lemma_list += [lemma]\n",
        "    tokens_stem += [lemma_list]\n",
        "print(tokens)\n",
        "print(tokens_stem)\n",
        " "
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['You', 'are', 'good', 'friends', '.'], ['We', 'had', 'two', 'houses', 'that', 'are', 'bigger', '!']]\n",
            "[['You', 'be', 'good', 'friend', '.'], ['We', 'have', 'two', 'house', 'that', 'be', 'big', '!']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "C8ZPSfBZ-L9Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}